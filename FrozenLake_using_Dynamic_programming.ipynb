{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diliprc96/Dynamic-Programming/blob/Dev/FrozenLake_using_Dynamic_programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlYxPEMMTYH"
      },
      "source": [
        "## Group No 160\n",
        "\n",
        "## Group Member Names:\n",
        "1. Dilip R 2023ad05030\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkXPirB1MTYK"
      },
      "source": [
        "1.**Problem statement**:\n",
        "\n",
        "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
        "\n",
        "2.**Scenario**:\n",
        "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
        "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected.\n",
        "\n",
        "\n",
        "#### Objective\n",
        "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
        "\n",
        "#### About the environment\n",
        "\n",
        "The environment consists of several types of tiles:\n",
        "* Start (S): The initial position of the agent, safe to step.\n",
        "* Frozen Tiles (F): Frozen surface, safe to step.\n",
        "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
        "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
        "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game.\n",
        "\n",
        "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
        "The agent earns rewards as follows:\n",
        "* Reaching the goal (G): +10 reward.\n",
        "* Falling into a hole (H): -10 reward.\n",
        "* Collecting a treasure (T): +5 reward.\n",
        "* Stepping on a frozen tile (F): 0 reward.\n",
        "\n",
        "#### States\n",
        "* Current position of the agent (row, column).\n",
        "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
        "\n",
        "#### Actions\n",
        "* Four possible moves: up, down, left, right\n",
        "\n",
        "#### Rewards\n",
        "* Goal (G): +10.\n",
        "* Treasure (T): +5 per treasure.\n",
        "* Hole (H): -10.\n",
        "* Frozen tiles (F): 0.\n",
        "\n",
        "#### Environment\n",
        "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
        "Example grid:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7NLPkoeMTYL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdkwnNHVMTYL"
      },
      "source": [
        "**Expected Outcomes:**\n",
        "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
        "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
        "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
        "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
        "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hInG_-M-MTYM"
      },
      "source": [
        "### Import required libraries and Define the custom environment - 2 Marks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WEnsE38MTYM"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnnQnoh5MTYN"
      },
      "outputs": [],
      "source": [
        "# Custom environment to create the given grid and respective functions that are required for the problem\n",
        "\n",
        "#Include functions to take an action, get reward, to check if episode is over\n",
        "\n",
        "class TreasureHuntFrozenLakeEnv(FrozenLakeEnv):\n",
        "    def __init__(self, desc=None, map_name=\"5x5\", is_slippery=True):\n",
        "        # Define the 5x5 map with treasures (T)\n",
        "        desc = np.array([\n",
        "            \"SFFHT\",\n",
        "            \"FHFFF\",\n",
        "            \"FFFTF\",\n",
        "            \"TFFHF\",\n",
        "            \"FFFFG\"\n",
        "        ], dtype=\"c\")\n",
        "        super().__init__(desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
        "        self.treasure_positions = {(0, 4), (2, 3), (3, 0)}  # Treasure positions in the given map\n",
        "        self.nS = self.observation_space.n\n",
        "        self.nA = self.action_space.n\n",
        "        self.holes = self.desc.flatten() == b'H'\n",
        "        self.goal =  np.where(self.desc.flatten() == b'G')[0][0]\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = super().step(action)\n",
        "\n",
        "        # Combine terminated and truncated into a single 'done' flag\n",
        "        done = terminated or truncated\n",
        "\n",
        "        row, col = divmod(obs, self.ncol)\n",
        "\n",
        "        # If the agent steps on a treasure\n",
        "        if (row, col) in self.treasure_positions:\n",
        "            reward += 5  # Award +5 for treasure\n",
        "            self.desc[row, col] = b'F'  # Convert treasure tile to frozen tile\n",
        "            self.treasure_positions.remove((row, col))\n",
        "\n",
        "        # Return the expected 4 values\n",
        "        return obs, reward, done, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeGFU2-xMTYN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dwc-TiNMTYO"
      },
      "source": [
        "### Value Iteration Algorithm - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYQeYfDrMTYO"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
        "    value_function = np.zeros(env.nS)  # Initialize value function\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.nS):\n",
        "            if state in env.holes or state == env.goal:\n",
        "                continue  # Skip terminal states\n",
        "            v = value_function[state]\n",
        "            value_function[state] = max(compute_q_value(env, value_function, state, action, gamma)\n",
        "                                        for action in range(env.nA))\n",
        "            delta = max(delta, abs(v - value_function[state]))\n",
        "\n",
        "        if delta < theta:  # Stop if value function converges\n",
        "            break\n",
        "    return value_function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoLU7kaMTYO"
      },
      "source": [
        "### Policy Improvement Function - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdIpbJERMTYO"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, value_function, gamma=0.99):\n",
        "    policy = np.zeros(env.nS, dtype=int)\n",
        "\n",
        "    for state in range(env.nS):\n",
        "        if state in env.holes or state == env.goal:\n",
        "            continue\n",
        "        policy[state] = np.argmax([compute_q_value(env, value_function, state, action, gamma)\n",
        "                                   for action in range(env.nA)])\n",
        "    return policy\n",
        "\n",
        "def compute_q_value(env, value_function, state, action, gamma):\n",
        "    q_value = 0\n",
        "    for prob, next_state, reward, done in env.P[state][action]:\n",
        "        q_value += prob * (reward + gamma * value_function[next_state] * (not done))\n",
        "    return q_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspm5RgBMTYO"
      },
      "source": [
        "### Print the Optimal Value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBJyy5M7MTYO"
      },
      "outputs": [],
      "source": [
        "def print_value_function(value_function, env):\n",
        "    print(\"Optimal Value Function:\")\n",
        "    grid = np.round(value_function.reshape(env.nrow, env.ncol), 2)\n",
        "    print(grid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpfNYA2KMTYO"
      },
      "source": [
        "### Visualization of the learned optimal policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZdIwHrLMTYO"
      },
      "outputs": [],
      "source": [
        "def visualize_policy(policy, env):\n",
        "    actions = ['←', '↓', '→', '↑']  # Left, Down, Right, Up\n",
        "    policy_grid = np.array([actions[action] for action in policy]).reshape(env.nrow, env.ncol)\n",
        "    print(\"Learned Optimal Policy:\")\n",
        "    print(policy_grid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NzFdsOoMTYP"
      },
      "source": [
        "### Evaluate the policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a6kkxy_MTYP"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(env, policy, episodes=1000):\n",
        "    total_reward = 0\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "        total_reward += episode_reward\n",
        "    avg_reward = total_reward / episodes\n",
        "    print(f\"Average Reward over {episodes} episodes: {avg_reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB7fi4OyMTYP"
      },
      "source": [
        "### Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxxMINSHMTYP",
        "outputId": "c130d0b3-a6f1-4be1-c035-f3e612491dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[[0.   0.   0.   0.   0.77]\n",
            " [0.36 0.   0.46 0.66 0.79]\n",
            " [0.74 0.74 0.74 0.74 0.84]\n",
            " [0.75 0.77 0.77 0.   0.91]\n",
            " [0.77 0.79 0.84 0.91 0.  ]]\n",
            "Learned Optimal Policy:\n",
            "[['←' '←' '←' '←' '→']\n",
            " ['←' '←' '↓' '↓' '→']\n",
            " ['↓' '↓' '↓' '↑' '→']\n",
            " ['↓' '↓' '←' '←' '→']\n",
            " ['↓' '↓' '↓' '↓' '←']]\n",
            "Average Reward over 1000 episodes: 1.008\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize the custom environment\n",
        "    env = TreasureHuntFrozenLakeEnv()\n",
        "\n",
        "    # Step 1: Perform Value Iteration\n",
        "    optimal_value_function = value_iteration(env)\n",
        "\n",
        "    # Step 2: Improve Policy based on the Value Function\n",
        "    optimal_policy = policy_improvement(env, optimal_value_function)\n",
        "\n",
        "    # Step 3: Print the Optimal Value Function\n",
        "    print_value_function(optimal_value_function, env)\n",
        "\n",
        "    # Step 4: Visualize the Learned Policy\n",
        "    visualize_policy(optimal_policy, env)\n",
        "\n",
        "    # Step 5: Evaluate the Policy\n",
        "    evaluate_policy(env, optimal_policy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym.envs.toy_text import FrozenLakeEnv\n",
        "\n",
        "class TreasureHuntFrozenLakeEnv(FrozenLakeEnv):\n",
        "    def __init__(self, desc=None, map_name=\"5x5\", is_slippery=True):\n",
        "        # Define the 5x5 map with treasures (T)\n",
        "        desc = np.array([\n",
        "            \"SFFHT\",\n",
        "            \"FHFFF\",\n",
        "            \"FFFTF\",\n",
        "            \"TFHFF\",\n",
        "            \"FFFFG\"\n",
        "        ], dtype=\"c\")\n",
        "        super().__init__(desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
        "        self.treasure_positions = {(0, 4), (2, 3), (3, 0)}  # Treasure positions in the given map\n",
        "        self.nS = self.observation_space.n\n",
        "        self.nA = self.action_space.n\n",
        "        self.holes = self.desc.flatten() == b'H'\n",
        "        self.goal =  np.where(self.desc.flatten() == b'G')[0][0]\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = super().step(action)\n",
        "\n",
        "        # Combine terminated and truncated into a single 'done' flag\n",
        "        done = terminated or truncated\n",
        "\n",
        "        row, col = divmod(obs, self.ncol)\n",
        "\n",
        "        # If the agent steps on a treasure\n",
        "        if (row, col) in self.treasure_positions:\n",
        "            reward += 5  # Award +5 for treasure\n",
        "            self.desc[row, col] = b'F'  # Convert treasure tile to frozen tile\n",
        "            self.treasure_positions.remove((row, col))\n",
        "\n",
        "        # If the agent reaches the goal\n",
        "        if (row, col) == (4, 4):\n",
        "            reward += 10  # Goal reward\n",
        "\n",
        "        # Return the expected 4 values\n",
        "        return obs, reward, done, info\n",
        "\n",
        "def compute_q_value(env, value_function, state, action, gamma):\n",
        "    q_value = 0\n",
        "    for prob, next_state, reward, done in env.P[state][action]:\n",
        "        q_value += prob * (reward + gamma * value_function[next_state] * (not done))\n",
        "    return q_value\n",
        "\n",
        "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
        "    value_function = np.zeros(env.nS)  # Initialize value function\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.nS):\n",
        "            if state in env.holes or state == env.goal:\n",
        "                continue  # Skip terminal states\n",
        "            v = value_function[state]\n",
        "            value_function[state] = max(compute_q_value(env, value_function, state, action, gamma)\n",
        "                                        for action in range(env.nA))\n",
        "            delta = max(delta, abs(v - value_function[state]))\n",
        "\n",
        "        if delta < theta:  # Stop if value function converges\n",
        "            break\n",
        "    return value_function\n",
        "\n",
        "def policy_improvement(env, value_function, gamma=0.99):\n",
        "    policy = np.zeros(env.nS, dtype=int)\n",
        "\n",
        "    for state in range(env.nS):\n",
        "        if state in env.holes or state == env.goal:\n",
        "            continue\n",
        "        policy[state] = np.argmax([compute_q_value(env, value_function, state, action, gamma)\n",
        "                                   for action in range(env.nA)])\n",
        "    return policy\n",
        "\n",
        "def print_value_function(value_function, env):\n",
        "    print(\"Optimal Value Function:\")\n",
        "    grid = np.round(value_function.reshape(env.nrow, env.ncol), 2)\n",
        "    print(grid)\n",
        "\n",
        "def visualize_policy(policy, env):\n",
        "    actions = ['←', '↓', '→', '↑']  # Left, Down, Right, Up\n",
        "    policy_grid = np.array([actions[action] for action in policy]).reshape(env.nrow, env.ncol)\n",
        "    print(\"Learned Optimal Policy:\")\n",
        "    print(policy_grid)\n",
        "\n",
        "# Main script to execute value iteration and policy improvement\n",
        "env = TreasureHuntFrozenLakeEnv()\n",
        "\n",
        "# Perform value iteration\n",
        "value_function = value_iteration(env)\n",
        "print_value_function(value_function, env)\n",
        "\n",
        "# Improve the policy using the value function\n",
        "policy = policy_improvement(env, value_function)\n",
        "visualize_policy(policy, env)\n",
        "\n",
        "# Evaluate the policy by running multiple episodes\n",
        "num_episodes = 1000\n",
        "total_rewards = []\n",
        "for _ in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "average_reward = np.mean(total_rewards)\n",
        "print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n"
      ],
      "metadata": {
        "id": "Pf6SHSLNG-uL",
        "outputId": "99cd2e36-cc2d-4b08-817d-50a87cd0dce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[[0.   0.   0.27 0.   0.79]\n",
            " [0.36 0.   0.55 0.72 0.81]\n",
            " [0.73 0.71 0.69 0.81 0.86]\n",
            " [0.76 0.75 0.   0.88 0.93]\n",
            " [0.79 0.81 0.86 0.93 0.  ]]\n",
            "Learned Optimal Policy:\n",
            "[['←' '←' '←' '←' '→']\n",
            " ['←' '←' '→' '↓' '→']\n",
            " ['↓' '↓' '↑' '→' '↓']\n",
            " ['↓' '←' '←' '→' '↓']\n",
            " ['↓' '↓' '↓' '→' '←']]\n",
            "Average Reward over 1000 episodes: 10.575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XR7pE2yBHYfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "coiC4atPHc4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSFQynRhMTYP"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}