{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diliprc96/Dynamic-Programming/blob/main/FrozenLake_using_Dynamic_programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlYxPEMMTYH"
      },
      "source": [
        "## Group No 160\n",
        "\n",
        "## Group Member Names:\n",
        "1. Dilip R 2023ad05030\n",
        "2.\n",
        "3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkXPirB1MTYK"
      },
      "source": [
        "1.**Problem statement**:\n",
        "\n",
        "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
        "\n",
        "2.**Scenario**:\n",
        "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
        "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected.\n",
        "\n",
        "\n",
        "#### Objective\n",
        "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
        "\n",
        "#### About the environment\n",
        "\n",
        "The environment consists of several types of tiles:\n",
        "* Start (S): The initial position of the agent, safe to step.\n",
        "* Frozen Tiles (F): Frozen surface, safe to step.\n",
        "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
        "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
        "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game.\n",
        "\n",
        "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
        "The agent earns rewards as follows:\n",
        "* Reaching the goal (G): +10 reward.\n",
        "* Falling into a hole (H): -10 reward.\n",
        "* Collecting a treasure (T): +5 reward.\n",
        "* Stepping on a frozen tile (F): 0 reward.\n",
        "\n",
        "#### States\n",
        "* Current position of the agent (row, column).\n",
        "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
        "\n",
        "#### Actions\n",
        "* Four possible moves: up, down, left, right\n",
        "\n",
        "#### Rewards\n",
        "* Goal (G): +10.\n",
        "* Treasure (T): +5 per treasure.\n",
        "* Hole (H): -10.\n",
        "* Frozen tiles (F): 0.\n",
        "\n",
        "#### Environment\n",
        "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
        "Example grid:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7NLPkoeMTYL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdkwnNHVMTYL"
      },
      "source": [
        "**Expected Outcomes:**\n",
        "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
        "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
        "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
        "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
        "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hInG_-M-MTYM"
      },
      "source": [
        "### Import required libraries and Define the custom environment - 2 Marks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WEnsE38MTYM"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
        "\n",
        "class TreasureHuntFrozenLakeEnv(FrozenLakeEnv):\n",
        "    def __init__(self, desc=None, map_name=\"5x5\", is_slippery=True):\n",
        "        # Define the 5x5 map with treasures (T)\n",
        "        desc = np.array([\n",
        "            \"SFHHT\",\n",
        "            \"FHFFF\",\n",
        "            \"FFFTF\",\n",
        "            \"TFFHF\",\n",
        "            \"FFFFG\"\n",
        "        ], dtype=\"c\")\n",
        "        super().__init__(desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
        "        self.treasure_positions = {(0, 4), (2, 3), (3, 0)}  # Treasure positions\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = super().step(action)\n",
        "        row, col = divmod(obs, self.ncol)\n",
        "\n",
        "        # If the agent steps on a treasure\n",
        "        if (row, col) in self.treasure_positions:\n",
        "            reward += 5  # Award +5 for treasure\n",
        "            self.desc[row, col] = b'F'  # Convert treasure tile to frozen tile\n",
        "            self.treasure_positions.remove((row, col))\n",
        "        return obs, reward, done, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnnQnoh5MTYN"
      },
      "outputs": [],
      "source": [
        "# Custom environment to create the given grid and respective functions that are required for the problem\n",
        "\n",
        "#Include functions to take an action, get reward, to check if episode is over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeGFU2-xMTYN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dwc-TiNMTYO"
      },
      "source": [
        "### Value Iteration Algorithm - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYQeYfDrMTYO"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
        "    value_function = np.zeros(env.nS)  # Initialize value function\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(env.nS):\n",
        "            if state in env.holes or state == env.goal:\n",
        "                continue  # Skip terminal states\n",
        "            v = value_function[state]\n",
        "            value_function[state] = max(compute_q_value(env, value_function, state, action, gamma)\n",
        "                                        for action in range(env.nA))\n",
        "            delta = max(delta, abs(v - value_function[state]))\n",
        "\n",
        "        if delta < theta:  # Stop if value function converges\n",
        "            break\n",
        "    return value_function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoLU7kaMTYO"
      },
      "source": [
        "### Policy Improvement Function - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdIpbJERMTYO"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, value_function, gamma=0.99):\n",
        "    policy = np.zeros(env.nS, dtype=int)\n",
        "\n",
        "    for state in range(env.nS):\n",
        "        if state in env.holes or state == env.goal:\n",
        "            continue\n",
        "        policy[state] = np.argmax([compute_q_value(env, value_function, state, action, gamma)\n",
        "                                   for action in range(env.nA)])\n",
        "    return policy\n",
        "\n",
        "def compute_q_value(env, value_function, state, action, gamma):\n",
        "    q_value = 0\n",
        "    for prob, next_state, reward, done in env.P[state][action]:\n",
        "        q_value += prob * (reward + gamma * value_function[next_state] * (not done))\n",
        "    return q_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspm5RgBMTYO"
      },
      "source": [
        "### Print the Optimal Value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBJyy5M7MTYO"
      },
      "outputs": [],
      "source": [
        "def print_value_function(value_function, env):\n",
        "    print(\"Optimal Value Function:\")\n",
        "    grid = np.round(value_function.reshape(env.nrow, env.ncol), 2)\n",
        "    print(grid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpfNYA2KMTYO"
      },
      "source": [
        "### Visualization of the learned optimal policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZdIwHrLMTYO"
      },
      "outputs": [],
      "source": [
        "def visualize_policy(policy, env):\n",
        "    actions = ['←', '↓', '→', '↑']  # Left, Down, Right, Up\n",
        "    policy_grid = np.array([actions[action] for action in policy]).reshape(env.nrow, env.ncol)\n",
        "    print(\"Learned Optimal Policy:\")\n",
        "    print(policy_grid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NzFdsOoMTYP"
      },
      "source": [
        "### Evaluate the policy - 1 Mark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a6kkxy_MTYP"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(env, policy, episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "        total_reward += episode_reward\n",
        "    avg_reward = total_reward / episodes\n",
        "    print(f\"Average Reward over {episodes} episodes: {avg_reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB7fi4OyMTYP"
      },
      "source": [
        "### Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxxMINSHMTYP"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize the custom environment\n",
        "    env = TreasureHuntFrozenLakeEnv()\n",
        "\n",
        "    # Step 1: Perform Value Iteration\n",
        "    optimal_value_function = value_iteration(env)\n",
        "\n",
        "    # Step 2: Improve Policy based on the Value Function\n",
        "    optimal_policy = policy_improvement(env, optimal_value_function)\n",
        "\n",
        "    # Step 3: Print the Optimal Value Function\n",
        "    print_value_function(optimal_value_function, env)\n",
        "\n",
        "    # Step 4: Visualize the Learned Policy\n",
        "    visualize_policy(optimal_policy, env)\n",
        "\n",
        "    # Step 5: Evaluate the Policy\n",
        "    evaluate_policy(env, optimal_policy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSFQynRhMTYP"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}